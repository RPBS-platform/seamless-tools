#!/usr/bin/env python3

import os
import csv
import cson
import json
from seamless import calculate_checksum
from seamless.core.protocol.serialize import serialize_sync as serialize

def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs)
    exit(1)

def load_csv(filename):
    result = {}
    with open(filename, newline='') as csvfile:
        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result

SDB = os.environ.get("SEAMLESS_DATABASE_DIR")
if SDB is None:
    err("SEAMLESS_DATABASE_DIR undefined")
FD = os.environ.get("FAIRSERVER_DIR")
if FD is None:
    err("FAIRSERVER_DIR undefined")

import argparse
parser = argparse.ArgumentParser(description="""Adds an distribution to a FAIR dataset.
The distribution refers to deepcell/deepfolder collection as it currently is in the database,
tagging it with a version or date.
""")

parser.add_argument(
    "dataset_name",
    help="Name of the FAIR dataset"
)

parser.add_argument(
    "collection_name",
    help="Collection name in SEAMLESS_DATABASE_DIR. Must be a deepcell or deepfolder"
)

parser.add_argument(
    "--version",
    help="Version tag of the added distribution. Only required if there is no date."
)

parser.add_argument(
    "--date",
    help="Date tag of the added distribution. Only required if there is no version."
)

parser.add_argument(
    "--format",
    help="""Format tag of the added distribution. 
This is just descriptive, allowing the same deepfolder to be in the same FAIR dataset
with distributions for multiple different file formats.
"""
)

parser.add_argument(
    "--compression",
    help="""Compression that describes the deepfolder. Can be gzip, zip or bzip.
Seamless does not automatically decompress such deepfolders, but command line tools
may expect a directory containing compressed files.
"""
)

parser.add_argument(
    "--merge-access-index",
    dest="merge_access_index",
    action="store_true",
    help="""Merge access indices with the current latest distribution.
If set, access index (if it exists) will be merged with existing access index 
in FAIRSERVER_DIR for the distribution that is latest  (given the specified format 
and compression).
If not, a new access index is built."""
)

parser.add_argument(
    "--no-latest",
    dest="no_latest",
    action="store_true",
    help="""Do not mark the added distribution as "latest". 
The latest distribution is what is returned to a FAIRserver request that
does not explicitly describe version or date."""
)

args = parser.parse_args()
if args.date is None and args.version is None:
    err("You must define --date and/or --version")

if args.compression not in (None, "gzip", "zip", "bzip"):
    err("If defined, compression must be gzip, zip or bzip")

deep_buffer_file1 = os.path.join(SDB, "deepcells", args.collection_name + ".json")
deep_buffer_file2 = os.path.join(SDB, "deepfolders", args.collection_name + ".json")
if not os.path.exists(deep_buffer_file1) and not os.path.exists(deep_buffer_file2):
    err("""Collection name does not exist or is not a deepcell or deepfolder.
Neither file exists:
  {}
  {}""".format(deep_buffer_file1, deep_buffer_file2))

if os.path.exists(deep_buffer_file1) and os.path.exists(deep_buffer_file2):
    err("""Malformed database directory structure.
Both files exist:
  {}
  {}""".format(deep_buffer_file1, deep_buffer_file2))

# Get deep buffer from $SDB/<deepcell-or-deepfolder>/<collection_name>.json. 
# Calculate checksum.

if os.path.exists(deep_buffer_file1):
    deep_buffer_file = deep_buffer_file1
    distribution_type = "deepcell" 
else:
    deep_buffer_file = deep_buffer_file2
    distribution_type = "deepfolder" 
with open(deep_buffer_file, "rb") as f:
    deep_buffer = f.read()
deep_buffer_size = len(deep_buffer)
deep_buffer_checksum = calculate_checksum(deep_buffer, hex=True)
deep_buffer_dict = json.loads(deep_buffer.decode())
distribution_nkeys = len(deep_buffer_dict)

# Copy deep buffer into $FD/deepbuffer/<checksum>
with open(os.path.join(FD, "deepbuffer", deep_buffer_checksum), "wb") as f:
    f.write(deep_buffer)

# Get deep buffer content length from deepcontent.csv
deepcontent = load_csv(os.path.join(SDB, "deep_indices", "deepcontent.csv"))
content_size = int(deepcontent[deep_buffer_checksum])


# If exists, load existing distributions from $FD/dataset_distributions/<dataset_name>
distributions_file = os.path.join(FD, "dataset_distributions", args.dataset_name + ".json")
if os.path.exists(distributions_file):
    with open(distributions_file) as f:
        # Load with CSON to be more robust towards manual editing
        distributions = cson.load(f)
        assert isinstance(distributions, list)
else:
    distributions = []

# Check that there is no distribution with the exact same version and date
for n, distribution in enumerate(distributions.copy()):
    if distribution["type"] != distribution_type:
        continue
    if distribution.get("format") != args.format:
        continue
    if distribution.get("compression") != args.compression:
        continue
    if distribution.get("version") != args.version:
        continue
    if distribution.get("date") != args.date:
        continue
    err("Distribution {} has the exact same version/date".format(n+1))

# Get all compatible distributions (same type, format and compression)
compatible_distributions = []
for distribution in distributions:
    if distribution["type"] != distribution_type:
        continue
    if distribution.get("format") != args.format:
        continue
    if distribution.get("compression") != args.compression:
        continue
    compatible_distributions.append(distribution)

# Get the latest distribution from the compatible distributions
for distribution in compatible_distributions:
    if not distribution.get("latest"):
        continue
    latest_distribution = distribution
    break
else:
    latest_distribution = None

"""
Build the keyorder. Take the keyorder from the latest distribution as basis.
Then add/delete new keys from the current distribution (sorted alphabetically). 
To maximize chunk coherence, fill up deleted keys:
  - preferably, with a new key from the current distribution
  - else, with an existing key from the latest distribution
Add all remaining new keys from the current distribution to the end.
Convert the keyorder list to Seamless JSON.
Calculate the checksum of the keyorder buffer and store the buffer in 
$FD/keyorder/<key order checksum>
"""
print("Determine key order...")
latest_keyorder = []
if latest_distribution is not None:
    latest_keyorder_checksum = latest_distribution.get("keyorder")
    if latest_keyorder_checksum is not None:
        with open(os.path.join(FD, "keyorder", latest_keyorder_checksum)) as f:
            latest_keyorder = json.load(f)
    else:
        latest_distribution_checksum = latest_distribution["checksum"]
        with open(os.path.join(FD, "deepbuffer", latest_distribution_checksum)) as f:
            latest_distribution_deep_buffer = json.load(f)
            latest_keyorder = sorted(list(latest_distribution_deep_buffer))
latest_keyorder_set = set(latest_keyorder)

added_keys = [key for key in sorted(deep_buffer_dict.keys()) if key not in latest_keyorder_set]

keyorder = []
added_key_pos = 0
latest_keyorder_size = len(latest_keyorder)
pos = 0
while pos < latest_keyorder_size:
    key = latest_keyorder[pos]
    pos += 1
    if key in deep_buffer_dict:
        keyorder.append(key)
    else:
        if added_key_pos < len(added_keys):
            added_key = added_keys[added_key_pos]
            keyorder.append(added_key)
            added_key_pos += 1
        else:
            final_key = None
            while pos < latest_keyorder_size - 1:
                final_key = latest_keyorder[latest_keyorder_size-1]
                latest_keyorder_size -= 1
                if final_key in deep_buffer_dict:
                    break
            if final_key is not None:
                keyorder.append(final_key)
keyorder += added_keys[added_key_pos:]        

keyorder_buffer = serialize(keyorder, "plain")
keyorder_checksum = calculate_checksum(keyorder_buffer, hex=True)
print("Key order checksum: {}".format(keyorder_checksum))
with open(os.path.join(FD, "keyorder", keyorder_checksum), "wb") as f:
    f.write(keyorder_buffer)

# If exists, load $SDB/download_index/<collection_name>.json as the raw access index

raw_access_index = None
raw_access_index_file = os.path.join(SDB, "download_indices", args.collection_name + ".json")
if os.path.exists(raw_access_index_file):
    with open(raw_access_index_file, "r") as f:
        raw_access_index = json.load(f)

"""
If --merge-access-index, update the last access index.
Else, make a brand new access index.

The access index is built from the raw access index above
 by converting each of its key to a checksum using the deep buffer
Once the access index has been built, calculate its checksum and store it under 
 $FD/access_index/<access-index-checksum>
"""

if raw_access_index is not None:
    access_index = {}
    
    latest_access_index_checksum = None
    if args.merge_access_index and latest_distribution is not None:
        latest_access_index_checksum = latest_distribution.get("access_index") 
        if latest_access_index_checksum is not None:
            latest_access_index_file = os.path.join(FD, "access_index", latest_access_index_checksum)
            with open(latest_access_index_file) as f:
                latest_access_index = json.load(f)
            access_index = latest_access_index

    print("Building access index...")
    for key, value in raw_access_index.items():
        if key not in deep_buffer_dict:
            print("WARNING: unknown key {}".format(key))
            continue
        checksum = deep_buffer_dict[key]
        access_index[checksum] = value
    access_index_buffer = serialize(access_index, "plain")
    access_index_checksum = calculate_checksum(access_index_buffer, hex=True)
    print("Access index checksum: {}".format(access_index_checksum))
    with open(os.path.join(FD, "access_index", access_index_checksum), "wb") as f:
        f.write(access_index_buffer)

    """If --merge-access-index, delete the old access index.
    Any distributions that point to it, will use the new index instead
    """
    if latest_access_index_checksum is not None:
        for distribution in compatible_distributions:
            if distribution.get("access_index") == latest_access_index_checksum:
                distribution["access_index"] = access_index_checksum
        os.remove(latest_access_index_file)

"""
Build the distribution and add it to the previously loaded distributions in
$FD/dataset_distributions/<dataset_name> . An distribution contains essentially what is served, but in addition it contains the list of raw access index files.
"""
if latest_distribution is not None:
    latest_distribution.pop("latest")

# TODO: JSON-LD support (schemas.org)
new_distribution = {
    "checksum": deep_buffer_checksum,
    "type": distribution_type,
    "version": args.version,
    "date": args.date,
    "format": args.format,
    "compression": args.compression,
    "latest": None if args.no_latest else True,
    "nkeys": distribution_nkeys,
    "index_size": deep_buffer_size,
    "content_size": content_size,
    "keyorder": keyorder_checksum,
    "access_index": access_index_checksum,
}

for key in list(new_distribution.keys()):
    if not new_distribution[key]:
        new_distribution.pop(key)

print(json.dumps(new_distribution, indent=2))
distributions.append(new_distribution)
with open(distributions_file, "w") as f:
    json.dump(distributions, f, indent=2)
